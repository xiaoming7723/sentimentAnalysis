{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74365adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import load_model\n",
    "import re\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from num2words import num2words\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# import vectorizer\n",
    "with open('tfidf_stem_sen.pkl', 'rb') as file:\n",
    "    tfidf_sen = pickle.load(file)\n",
    "        \n",
    "# import model\n",
    "with open('Logistic Regression - S_Stem.pkl', 'rb') as file:\n",
    "    model_sen = pickle.load(file)\n",
    "    \n",
    "with open('Bernoulli Naive Bayes - Augment_Lemmatize_sar.pkl', 'rb') as file:\n",
    "    model_sar = pickle.load(file)\n",
    "\n",
    "with open('tfidf_augment_sar.pkl', 'rb') as file:\n",
    "    tfidf_sar = pickle.load(file)\n",
    "\n",
    "    \n",
    "keep_stopwords = {'not', 'no', \"don't\", \"didn't\", \"doesn't\", \"hadn't\", \"haven't\", \"isn't\",\n",
    "                  \"mustn't\", \"shouldn't\", \"wasn't\", \"weren't\", \"won't\", \"i\", \"you're\", \"you've\",\n",
    "                  \"you're\", \"you've\", \"you'll\", \"you'd\", \"she's\", \"that'll\", 'until', 'food',\n",
    "                  'place', 'service', 'good', 'nice', 'delicious', 'restaurant', 'eatery',\n",
    "                  'dining', 'experience', 'meal', 'menu', 'dish', 'cuisine', 'server', 'waiter',\n",
    "                  'waitress', 'staff', 'atmosphere', 'ambiance', 'decor', 'taste', 'flavor',\n",
    "                  'presentation', 'price', 'value', 'pricey', 'affordable', 'cost', 'money',\n",
    "                  'clean', 'hygiene', 'hygienic', 'spacious', 'cozy', 'crowded', 'busy', 'empty',\n",
    "                  'reservation', 'wait', 'waiting', 'quick', 'fast', 'slow', 'speed', 'fresh',\n",
    "                  'overcooked', 'undercooked', 'bland', 'spicy', 'salty', 'sweet', 'sour',\n",
    "                  'crispy', 'tender', 'juicy', 'portion', 'size', 'huge', 'small', 'large',\n",
    "                  'recommend', 'suggest', 'try', 'must', 'definitely', 'absolutely', 'probably',\n",
    "                  'likely', 'unlikely', 'maybe', 'perhaps', 'probably', 'possibly', 'certainly',\n",
    "                  'surely', 'love', 'like', 'dislike', 'hate', 'enjoy', 'prefer', 'favor',\n",
    "                  'impressed', 'satisfied', 'happy', 'unhappy', 'disappointed', 'pleasant',\n",
    "                  'unpleasant', 'friendly', 'rude', 'polite', 'attentive', 'negligent', 'helpful',\n",
    "                  'accommodating', 'knowledgeable', 'courteous', 'recommendation', 'favorite','good','ok','nice','fast'}\n",
    "\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "stopwords_list -= keep_stopwords\n",
    "\n",
    "abbreviations = {'fyi': 'for your information',\n",
    "                 'lol': 'laugh out loud',\n",
    "                 'loza': 'laughs out loud',\n",
    "                 'lmao': 'laughing',\n",
    "                 'rofl': 'rolling on the floor laughing',\n",
    "                 'vbg': 'very big grin',\n",
    "                 'xoxo': 'hugs and kisses',\n",
    "                 'xo': 'hugs and kisses',\n",
    "                 'brb': 'be right back',\n",
    "                 'tyt': 'take your time',\n",
    "                 'thx': 'thanks',\n",
    "                 'abt': 'about',\n",
    "                 'bf': 'best friend',\n",
    "                 'diy': 'do it yourself',\n",
    "                 'faq': 'frequently asked questions',\n",
    "                 'fb': 'facebook',\n",
    "                 'idk': 'i don\\'t know',\n",
    "                 'asap': 'as soon as possible',\n",
    "                 'syl': 'see you later',\n",
    "                 'nvm': 'never mind',\n",
    "                 'frfr':'for real for real',\n",
    "                 'istg':'i swear to god',\n",
    "    }\n",
    "\n",
    "# remove punctuations, numbers, and stopwords\n",
    "def clean_text_sar(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    text = pattern.sub('', text)\n",
    "    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n",
    "    emoji = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = emoji.sub(r'', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)        \n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text) \n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)  \n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)  \n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"did't\", \"did not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "    text = re.sub(r\"have't\", \"have not\", text)\n",
    "    text = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# remove punctuations, numbers, and stopwords\n",
    "def clean_text(sentences):\n",
    "    # convert text to lowercase\n",
    "    text = sentences.lower()\n",
    "\n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    text = pattern.sub('', text)\n",
    "    \n",
    "    # remove mentions\n",
    "    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n",
    "\n",
    "    # replace emoji with text\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(r'[:_]', ' ', text)\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # replace time stamp\n",
    "    time_reg = re.compile(\"(\\d{1,2}:)?\\d{1,2}:\\d{2}(\\.\\d{1,3})?\")\n",
    "    text = time_reg.sub(r'',text)\n",
    "\n",
    "    # contractions\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"did't\", \"did not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "    text = re.sub(r\"have't\", \"have not\", text)\n",
    "    \n",
    "    # abbreviation\n",
    "    text = replace_abbreviations(text)\n",
    "    \n",
    "    # Replace ‘’ with ''\n",
    "    text = re.sub(r'[‘’]','\\'', text)\n",
    "\n",
    "    # Replace “” with \"\"\n",
    "    text = text.replace('“', '\"').replace('”', '\"')\n",
    "\n",
    "    # remove text in square brackets\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "\n",
    "    # removing punctuations and replace with space\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "\n",
    "    # Substitute ordinal numbers like \"1st\", \"2nd\", \"3rd\" with words\n",
    "    text = re.sub(r'(\\d+)(st|nd|rd|th)\\b', lambda match: ordinal_to_word(match.group(1)), text)\n",
    "\n",
    "    # removing words containing digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # remove duplicated charcter to only one\n",
    "    text = re.sub(r'([a-zA-Z])\\1{2,}','\\1', text) \n",
    "\n",
    "    # Join the words\n",
    "    text = ' '.join([word for word in text.split()\n",
    "                     if word not in stopwords_list])\n",
    "    return text\n",
    "\n",
    "def ordinal_to_word(number_str):\n",
    "    # Convert the string representation of the number to an integer\n",
    "    number = int(number_str)\n",
    "\n",
    "    return(num2words(number, to =\"ordinal\"))\n",
    "\n",
    "def replace_abbreviations(text):\n",
    "    for abbr, full_form in abbreviations.items():\n",
    "        pattern = r\"\\b\" + re.escape(abbr) + r\"\\b\"\n",
    "        text = re.sub(pattern, full_form, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "def stem_sentence(text):\n",
    "    new_sen = []\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)\n",
    "        pos_tagged = pos_tag(words)\n",
    "        wordnet_tagged = list(map(lambda x: (x[0],pos_tagger(x[1])), pos_tagged)) # POS Tagging & Reducing\n",
    "        for word, tag in wordnet_tagged:\n",
    "            if tag is None:\n",
    "                new_sen.append(word)\n",
    "            else:\n",
    "                new_sen.append(porter_stemmer.stem(word,tag))                   # stemming\n",
    "    \n",
    "    return ' '.join(new_sen)\n",
    "\n",
    "def predict_sarcasm(text):\n",
    "    cleaned_text = clean_text_sar(text)\n",
    "    text_vectorized = tfidf_sar.transform([cleaned_text])\n",
    "    sarcasm_prediction = model_sar.predict(text_vectorized)\n",
    "    return sarcasm_prediction[0]\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    cleaned_text = clean_text(text)\n",
    "    stemmed_text = stem_sentence(cleaned_text)\n",
    "    text_vectorized = tfidf_sen.transform([stemmed_text])\n",
    "    sentiment_prediction = model_sen.predict(text_vectorized)\n",
    "    \n",
    "    return sentiment_prediction[0] \n",
    "\n",
    "\n",
    "enter_trigger = \"\"\"\n",
    "<script>\n",
    "document.addEventListener('keydown', function(event) {\n",
    "    if (event.code === 'Enter') {\n",
    "        document.querySelector('.streamlit-button').click();\n",
    "    }\n",
    "});\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "st.markdown(enter_trigger, unsafe_allow_html=True)\n",
    "\n",
    "st.title('Sentiment Analysis with Sarcasm Detection')\n",
    "\n",
    "# User input\n",
    "user_input = st.text_area('Enter your text here:')\n",
    "\n",
    "# Make prediction\n",
    "if st.button('Predict'):\n",
    "    if user_input:\n",
    "        text_for_sarcasm = user_input\n",
    "        predicted_sentiment = predict_sentiment(user_input)\n",
    "        \n",
    "        if(predicted_sentiment == 1.0):\n",
    "\n",
    "            if predict_sarcasm(user_input) == 1.0:\n",
    "                sarcasm = True\n",
    "            elif predict_sarcasm(user_input) == 0.0:\n",
    "                sarcasm = False\n",
    "            \n",
    "            if sarcasm:\n",
    "                st.write('Negative')\n",
    "            else:\n",
    "                st.write('Positive')\n",
    "            \n",
    "        else:\n",
    "            st.write('Negative') \n",
    "\n",
    "    else:\n",
    "        st.warning('Please enter some text before predicting.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2be3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! streamlit run app.py & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3b948d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
